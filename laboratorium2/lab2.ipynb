{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dane ustrukturyzowane\n",
    "\n",
    "Dane klienta to pewne wartości, które możesz przypisać do zmiennych: \n",
    "np wiek: 42, wzrost: 178, pozyczka: 1000, zarobki: 5000, imię: Jan\n",
    "\n",
    "Zdefiniuj zmienne customer_1_{cecha} i przypisz im wartości z przykładu powyżej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dlaczego do analizy danych nie używamy zmiennych? \n",
    "\n",
    "> Niezależnie od typu analizowanych i przetwarzanych danych w Pythonie możemy zebrać dane i reprezentować je jako pewna formy `listy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dlaczego listy nie są najlepszym miejscem na przechowywanie danych?\n",
    "\n",
    "Weźmy dwie listy numeryczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodawanie list\n",
    "print(f\"a+b: {a+b}\")\n",
    "# można też użyć metody format\n",
    "print(\"a+b: {}\".format(a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnożenie list\n",
    "try:\n",
    "    print(a*b)\n",
    "except TypeError:\n",
    "    print(\"no-defined operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Każdy obiekt pythonowy można rozszerzyć o nowe metody i atrybuty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "aa = np.array(a)\n",
    "bb = np.array(b)\n",
    "\n",
    "print(aa,bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"aa+bb: {aa+bb}\")\n",
    "# dodawanie działa\n",
    "try:\n",
    "    print(\"=\"*50)\n",
    "    print(aa*bb)\n",
    "    print(\"aa*bb - czy to poprawne mnożenie?\")\n",
    "    print(np.dot(aa,bb))\n",
    "    print(\"np.dot - a czy otrzymany wynik też realizuje poprawne mnożenie?\")\n",
    "except TypeError:\n",
    "    print(\"no-defined operation\")\n",
    "# mnożenie również działa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co działa szybciej?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iloczyn_skalarny_lista(x: list, y: list) -> float: \n",
    "    iloczyn = 0.\n",
    "    for i in range(len(x)):\n",
    "        iloczyn += x[i] * y[i]\n",
    "    return iloczyn\n",
    "\n",
    "a = list(range(1000))\n",
    "b = list(range(1000))\n",
    "\n",
    "%timeit iloczyn_skalarny_lista(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def iloczyn_skalarny_numpy(x, w):\n",
    "    return x.dot(w)\n",
    "    \n",
    "a = np.arange(1000)\n",
    "b = np.arange(1000)\n",
    "\n",
    "%timeit iloczyn_skalarny_numpy(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# własności tablic\n",
    "x = np.array(range(4))\n",
    "print(x)\n",
    "x.shape\n",
    "\n",
    "A = np.array([range(4),range(4)])\n",
    "# transposition  row i -> column j, column j -> row i \n",
    "A.T\n",
    "\n",
    "# 0-dim object\n",
    "scalar = np.array(5)\n",
    "print(f\"scalar object dim: {scalar.ndim}\")\n",
    "# 1-dim object\n",
    "vector_1d = np.array([3, 5, 7])\n",
    "print(f\"vector object dim: {vector_1d.ndim}\")\n",
    "# 2 rows for 3 features\n",
    "matrix_2d = np.array([[1,2,3],[3,4,5]])\n",
    "print(f\"matrix object dim: {matrix_2d.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczenia wykonywane na danych mieszczących się w pamięci. \n",
    "> czy można jeszcze przyśpieszyć obliczenia? \n",
    "\n",
    "\n",
    "\n",
    "[Kurs Numpy ze strony Sebastiana Raschki](https://sebastianraschka.com/blog/2020/numpy-intro.html)\n",
    "\n",
    "\n",
    "## PyTorch \n",
    "\n",
    "[PyTorch](https://pytorch.org) is an open-source Python-based deep learning library. \n",
    "PyTorch has been the most widely used deep learning library for research since 2019 by a wide margin. In short, for many practitioners and researchers, PyTorch offers just the right balance between usability and features.\n",
    "\n",
    "1. PyTorch is a `tensor` library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.\n",
    "\n",
    "2. PyTorch is an `automatic differentiation engine`, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.\n",
    "\n",
    "3. PyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "tensor0d = torch.tensor(1) \n",
    "tensor1d = torch.tensor([1, 2, 3])\n",
    "tensor2d = torch.tensor([[1, 2, 2], [3, 4, 5]])\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typy\n",
    "print(tensor1d.dtype)\n",
    "\n",
    "print(torch.tensor([1.0, 2.0, 3.0]).dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2d.shape\n",
    "print(tensor2d.reshape(3, 2))\n",
    "print(tensor2d.view(3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor2d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor2d.matmul(tensor2d.T))\n",
    "\n",
    "print(tensor2d @ tensor2d.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "szczegółowe info znajdziesz w [dokumentacji](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "## Modelowanie danych ustrukturyzowanych\n",
    "\n",
    "Rozważmy jedną zmienną (`xs`) od której zależy nasza zmienna wynikowa (`ys` - target).\n",
    "```python\n",
    "xs = np.array([-1,0,1,2,3,4])\n",
    "ys = np.array([-3,-1,1,3,5,7])\n",
    "```\n",
    "\n",
    "\n",
    "Modelem który możemy zastosować jest regresja liniowa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regresja liniowa \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "xs = np.array([-1,0,1,2,3,4])\n",
    "# a raczej \n",
    "xs = xs.reshape(-1, 1)\n",
    "\n",
    "ys = np.array([-3, -1, 1, 3, 5, 7])\n",
    "\n",
    "reg = LinearRegression()\n",
    "model = reg.fit(xs,ys)\n",
    "\n",
    "print(f\"solution: x1={model.coef_[0]}, x0={reg.intercept_}\")\n",
    "\n",
    "model.predict(np.array([[1],[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prosty kod realizuje w pełni nasze zadanie znalezienia modelu regresji liniowej. \n",
    "\n",
    "Do czego może nam posłużyc tak wygenerowany model? \n",
    "\n",
    "Aby z niego skorzystac potrzebujemy wyeksportować go do pliku.\n",
    "\n",
    "Wykorzystaj bibliotekę pickle w celu zapisu obiektu modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "with open('model.pkl', \"wb\") as picklefile:\n",
    "    pickle.dump(model, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możemy go zaimportować (np na Github) i wykorzystać w innych projektach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "with open('model.pkl',\"rb\") as picklefile:\n",
    "    mreg = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mreg.predict(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "x_t = torch.tensor([1,2,3,4,5]).view(-1,1)\n",
    "x_t=x_t.to(torch.float32)\n",
    "m = Linear(1,1)\n",
    "m(torch.tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward \n",
    "import torch.nn.functional as F\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b  = torch.tensor([0.2], requires_grad=True)\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(a,y)\n",
    "\n",
    "# automatic diff \n",
    "from torch.autograd import grad\n",
    "\n",
    "grad_L_w1 = grad(loss, w1, retain_graph= True)\n",
    "grad_L_b = grad(loss, b, retain_graph= True)\n",
    "\n",
    "loss.backward()\n",
    "print(w1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, n_input:int, n_output: int):\n",
    "        super(LinearRegression, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_input, n_output)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(xs, dtype=np.float32)\n",
    "y = np.array(ys, dtype=np.float32)\n",
    "\n",
    "X_train = torch.from_numpy(x).view(-1,1)\n",
    "y_train = torch.from_numpy(y).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression(1,1)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\n",
    "print(f\"liczba trenowalnych parametrów: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in lr_model.layers:\n",
    "    if isinstance(layer, torch.nn.Linear):\n",
    "        print(f\"weight: {layer.state_dict()['weight']}\")\n",
    "        print(f\"bias: {layer.state_dict()['bias']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# petla uczaca \n",
    "for epoch in range(epochs):\n",
    "    lr_model.train() # etap trenowania \n",
    "\n",
    "    y_pred = lr_model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.2f}')\n",
    " \n",
    "    lr_model.eval() # etap ewaluacji modelu\n",
    "\n",
    "# po treningu jeszcze raz generujemy predykcje\n",
    "lr_model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = lr_model(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.layers[0].weight, lr_model.layers[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inne sposoby pozyskiwania danych \n",
    "\n",
    "1. Gotowe źródła w bibliotekach pythonowych\n",
    "2. Dane z plików zewnętrznych (np. csv, json, txt) z lokalnego dysku lub z internetu\n",
    "3. Dane z bazy danych (np. MySQL, PostgreSQL, MongoDB)\n",
    "4. Dane generowane w sposób sztuczny pod wybrany problem modelowy. \n",
    "5. Strumienie danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# find all keys\n",
    "print(iris.keys())\n",
    "\n",
    "# print description\n",
    "print(iris.DESCR)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create DataFrame\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                  columns= iris['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show last\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info about NaN values and a type of each column.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new features\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features (columns) \n",
    "df = df.drop(columns=['target'])\n",
    "# filtering first 100 rows and 4'th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "iris_melt = pd.melt(df, \"species\", var_name=\"measurement\")\n",
    "f, ax = plt.subplots(1, figsize=(15,9))\n",
    "sns.stripplot(x=\"measurement\", y=\"value\", hue=\"species\", data=iris_melt, jitter=True, edgecolor=\"white\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:100,[0,2]].values\n",
    "y = df.iloc[0:100,4].values\n",
    "\n",
    "y = np.where(y == 'setosa',-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:50,0],X[:50,1],color='red', marker='o',label='setosa')\n",
    "plt.scatter(X[50:100,0],X[50:100,1],color='blue', marker='x',label='versicolor')\n",
    "plt.xlabel('sepal length (cm)')\n",
    "plt.ylabel('petal length (cm)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5],[4,5.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "źródła zewnętrzne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "col_names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
    "df = pd.read_csv(IRIS_PATH, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to sqlite\n",
    "import sqlite3\n",
    "# generate database\n",
    "conn = sqlite3.connect(\"iris.db\")\n",
    "# pandas to_sql\n",
    "\n",
    "try:\n",
    "    df.to_sql(\"iris\", conn, index=False)\n",
    "except:\n",
    "    print(\"tabela już istnieje\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_sql(\"SELECT * FROM iris WHERE sepal_length > 5\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sztuczne dane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane sztucznie generowane\n",
    "from sklearn import datasets\n",
    "X, y = datasets.make_classification(n_samples=10**4,\n",
    "n_features=20, n_informative=2, n_redundant=2)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# podział na zbiór treningowy i testowy\n",
    "train_samples = 7000 # 70% danych treningowych\n",
    "\n",
    "X_train = X[:train_samples]\n",
    "X_test = X[train_samples:]\n",
    "y_train = y[:train_samples]\n",
    "y_test = y[train_samples:]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "rfc.predict(X_train[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dane nieustruktyryzowane\n",
    "\n",
    "Dane nieustrukturyzowane to dane, które nie są w żaden sposób uporządkowane.\n",
    "\n",
    "1. obrazy \n",
    "2. teksty\n",
    "3. dźwięk\n",
    "4. wideo\n",
    "\n",
    "Niezależnie od typu wszystko przetwarzamy w tensorach (macierzach wielowymiarowych). Może to prowadzić do chęci używania modeli ML i sieci neuronowych do analizy danych nieustrukturyzowanych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "\n",
    "# 2-dim picture 28 x 28 pixel random \n",
    "picture_2d = # TWOJ KOD \n",
    "picture_2d[0:5,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(picture_2d, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist # 60000 obrazow 28x28\n",
    "(x_train_f, y_train_f),(x_test,y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, x_train = x_train_f[:5000]/255.0, x_train_f[5000:]/255.0\n",
    "y_valid, y_train = y_train_f[:5000], y_train_f[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "\n",
    "model.layers # dostęp do warstw modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train_f, y_train_f, epochs=5, validation_data = (x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)\n",
    "\n",
    "x_new = x_test[:3]\n",
    "y_pr = model.predict(x_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format json\n",
    "\n",
    "Twórz i zarządzaj jsonami w połączeniu z bazą danych mongoDB. \n",
    "Baza ta dostępna jest jako osobny mikroserwis w Dockerze. \n",
    "Przed podłączeniem sprawdź jak w pliku docker-compose.yml jest skonfigurowany serwis mongoDB (user i pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "person = '{\"name\": \"Alice\", \"languages\": [\"English\", \"French\"]}'\n",
    "person_dict = json.loads(person)\n",
    "\n",
    "print(person_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file test.json\n",
    "{\"name\": \"Alice\", \"languages\": [\"English\", \"French\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('person.json', 'w') as json_file:\n",
    "    json.dump(person_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
