# Apache Kafka - Wprowadzenie

Apache Kafka to system przetwarzania strumieniowego (event streaming), kt贸ry dziaa jako rozproszony broker wiadomoci. 
Pozwala na przesyanie i przetwarzanie danych w czasie rzeczywistym.

Domylnym adresem naszego brokera jest `broker:9092`.

W Apache Kafka dane s przechowywane w strukturach zwanych **topicami**, kt贸re peni funkcj kolejek komunikacyjnych.

Zarzdzanie Kafk odbywa si za pomoc skrypt贸w. W naszym przypadku bd to skrypty `.sh`.

## 1锔 Sprawd藕 list topic贸w
Pamitaj, aby przej do katalogu domowego:
```sh
cd ~
kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092
```

### 2锔 Utw贸rz nowy topic o nazwie `mytopic`
```sh
kafka/bin/kafka-topics.sh --create --topic mytopic --bootstrap-server broker:9092
```

### 3锔 Utw贸rz producenta
Ten skrypt pozwoli Ci wprowadza eventy rcznie przez terminal. Opcje `--property` s dodatkowe i su偶 do analizy w tym przykadzie.
```sh
kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic mytopic --property "parse.key=true" --property "key.separator=:"
```

### 4锔 Consumer w Sparku
Otw贸rz nowy terminal w miejscu, gdzie znajduje si plik `test_key_value.py`, i uruchom program **Consumer**a w Sparku.

```python
from pyspark.sql import SparkSession

KAFKA_BROKER = 'broker:9092'
KAFKA_TOPIC = 'mytopic'

spark = SparkSession.builder.getOrCreate()
spark.sparkContext.setLogLevel("WARN")

df = (spark.readStream.format("kafka")
      .option("kafka.bootstrap.servers", KAFKA_BROKER)
      .option("subscribe", KAFKA_TOPIC)
      .option("startingOffsets", "earliest")
      .load()
     )

# Konwersja danych binarnych na stringi
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
 .writeStream \
 .format("console") \
 .outputMode("append") \
 .start() \
 .awaitTermination()
```

Pamitaj, 偶e Apache Spark nie posiada domylnego konektora do Kafki, dlatego uruchom proces za pomoc `spark-submit` i pobierz odpowiedni pakiet w Scali:
```sh
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 test_key_value.py
```

### 5锔 Przetestuj przesyanie danych
W terminalu z uruchomionym producentem wpisz tekst w postaci:
```bash
jan:45
alicja:20
```

Sprawd藕, co pojawia si w oknie aplikacji Consumera.

### 6锔 Zakoczenie procesu
Po zakoczeniu pokazu u偶yj `Ctrl+C`, aby zamkn zar贸wno okno producenta, jak i aplikacj Spark.

---
Gotowe! Teraz masz podstawow konfiguracj Apache Kafka i Spark do analizy strumieniowej. 

